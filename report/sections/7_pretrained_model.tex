\section{Pretrained Model (DistilBERT)}
To see how good our model performs compared to a well-known model, we used the pretrained DistilBERT base model (uncased) from Hugging Face. The model is trained on a dataset containing 11,038 books and English Wikipedia having a vocabulary size of 30,000 words.

To fit the model to our task, we trained the model for 5 epochs using a learning rate of $5e-6$. The performance of the model can be seen in Table \ref{tab:distilbert_model}.
\begin{table}[H]
    \vspace*{-0.5cm}
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Test Accuracy & Test F1-Score \\ \hline
    88.9\% & 88.7\% \\ \hline
    \end{tabular}
    \caption{DistilBERT model performance on test set.}
    \label{tab:distilbert_model}
    \vspace*{-0.8cm}
\end{table}
